use crossbeam_channel::{bounded, Receiver, Sender};
use duckdb::{params, Connection, Result};
use flate2::read::GzDecoder;
use rayon::prelude::*;
use serde_json::Value;
use std::{
    fs::File,
    io::{BufRead, BufReader},
    sync::atomic::{AtomicUsize, Ordering},
    thread,
    time::Instant,
};

const BATCH_SIZE: usize = 50_000;
const CHUNK_SIZE: usize = 50_000; // Smaller chunks = more parallelism overlap
const CHANNEL_BUFFER: usize = 8; // Buffer size for channels

type Row = (
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
    Option<String>,
);

fn main() -> Result<()> {
    let input = "/media/tamil-07/New Volume1/torrents/gz/part-00001.gz";
    let output = "part-00001.parquet";

    let num_threads = rayon::current_num_threads();
    println!("ðŸš€ Using {} CPU cores for parallel processing", num_threads);
    println!("ðŸ“– Starting pipelined processing...\n");

    let start_time = Instant::now();
    let total_rows = AtomicUsize::new(0);

    // Channel: Reader -> Parser (sends chunks of raw lines)
    let (line_sender, line_receiver): (Sender<Vec<String>>, Receiver<Vec<String>>) =
        bounded(CHANNEL_BUFFER);

    // Channel: Parser -> Writer (sends parsed rows)
    let (row_sender, row_receiver): (Sender<Vec<Row>>, Receiver<Vec<Row>>) =
        bounded(CHANNEL_BUFFER);

    // ==================== READER THREAD ====================
    // Reads gzip file and sends chunks of lines to parser
    let reader_handle = thread::spawn(move || {
        let file = File::open(input).expect("Failed to open input file");
        let decoder = GzDecoder::new(file);
        let reader = BufReader::with_capacity(16 * 1024 * 1024, decoder); // 16MB buffer

        let mut chunk: Vec<String> = Vec::with_capacity(CHUNK_SIZE);
        let mut chunks_sent = 0;

        for line in reader.lines().flatten() {
            chunk.push(line);

            if chunk.len() >= CHUNK_SIZE {
                line_sender.send(chunk).expect("Failed to send chunk");
                chunk = Vec::with_capacity(CHUNK_SIZE);
                chunks_sent += 1;
            }
        }

        // Send remaining lines
        if !chunk.is_empty() {
            line_sender.send(chunk).expect("Failed to send final chunk");
            chunks_sent += 1;
        }

        drop(line_sender); // Signal end of input
        chunks_sent
    });

    // ==================== PARSER THREADS (via rayon) ====================
    // Receives chunks, parses JSON in parallel, sends parsed rows
    let parser_handle = thread::spawn(move || {
        let mut batches_sent = 0;

        for lines_chunk in line_receiver {
            // Parse JSON in parallel using all CPU cores
            let parsed_rows: Vec<Row> = lines_chunk
                .par_iter()
                .filter_map(|line| parse_json_line(line))
                .collect();

            if !parsed_rows.is_empty() {
                row_sender.send(parsed_rows).expect("Failed to send rows");
                batches_sent += 1;
            }
        }

        drop(row_sender); // Signal end of parsing
        batches_sent
    });

    // ==================== WRITER THREAD (main thread) ====================
    // Receives parsed rows and inserts into DuckDB
    let mut conn = Connection::open_in_memory()?;

    conn.execute_batch(
        r#"
        CREATE TABLE people (
            id TEXT,
            full_name TEXT,
            gender TEXT,
            job_title TEXT,
            location_country TEXT,
            location_region TEXT,
            location_continent TEXT,
            job_last_updated DATE,
            experience JSON,
            education JSON,
            profiles JSON,
            version_status JSON
        );
        "#,
    )?;

    let mut batch_num = 0;
    let mut last_report_time = Instant::now();
    let mut rows_since_last_report = 0;

    for parsed_batch in row_receiver {
        let batch_len = parsed_batch.len();
        total_rows.fetch_add(batch_len, Ordering::Relaxed);
        rows_since_last_report += batch_len;

        // Insert in sub-batches for optimal transaction size
        for sub_batch in parsed_batch.chunks(BATCH_SIZE) {
            insert_batch(&mut conn, sub_batch)?;
        }

        batch_num += 1;

        // Report progress every ~100k rows
        let current_total = total_rows.load(Ordering::Relaxed);
        if rows_since_last_report >= 100_000 {
            let elapsed = last_report_time.elapsed();
            let rows_per_sec = rows_since_last_report as f64 / elapsed.as_secs_f64();
            
            println!(
                "  âœ… Processed {:>7} rows | Batch #{:>2} | â±ï¸  {:.2}s | ðŸš€ {:.0} rows/sec",
                current_total,
                batch_num,
                elapsed.as_secs_f64(),
                rows_per_sec
            );
            
            last_report_time = Instant::now();
            rows_since_last_report = 0;
        }
    }

    // Wait for reader and parser to finish
    let chunks_read = reader_handle.join().expect("Reader thread panicked");
    let batches_parsed = parser_handle.join().expect("Parser thread panicked");

    let final_count = total_rows.load(Ordering::Relaxed);
    let total_elapsed = start_time.elapsed();

    println!("\nðŸ“Š Processing complete:");
    println!("   Total rows: {}", final_count);
    println!("   Chunks read: {}", chunks_read);
    println!("   Batches parsed: {}", batches_parsed);
    println!("   Total time: {:.2}s", total_elapsed.as_secs_f64());
    println!(
        "   Throughput: {:.0} rows/sec",
        final_count as f64 / total_elapsed.as_secs_f64()
    );

    println!("\nðŸ’¾ Writing Parquet file...");
    let parquet_start = Instant::now();
    conn.execute(
        &format!(
            "COPY people TO '{}' (FORMAT PARQUET, COMPRESSION ZSTD);",
            output
        ),
        [],
    )?;
    println!(
        "âœ… Parquet written: {} ({:.2}s)",
        output,
        parquet_start.elapsed().as_secs_f64()
    );

    Ok(())
}

/// Parse a single JSON line into a Row tuple
#[inline]
fn parse_json_line(line: &str) -> Option<Row> {
    if let Ok(Value::Object(obj)) = serde_json::from_str::<Value>(line) {
        Some((
            obj.get("id").and_then(|v| v.as_str()).map(|s| s.to_string()),
            obj.get("full_name").and_then(|v| v.as_str()).map(|s| s.to_string()),
            obj.get("gender").and_then(|v| v.as_str()).map(|s| s.to_string()),
            obj.get("job_title").and_then(|v| v.as_str()).map(|s| s.to_string()),
            obj.get("location_country").and_then(|v| v.as_str()).map(|s| s.to_string()),
            obj.get("location_region").and_then(|v| v.as_str()).map(|s| s.to_string()),
            obj.get("location_continent").and_then(|v| v.as_str()).map(|s| s.to_string()),
            obj.get("job_last_updated").and_then(|v| v.as_str()).map(|s| s.to_string()),
            obj.get("experience").map(|v| v.to_string()),
            obj.get("education").map(|v| v.to_string()),
            obj.get("profiles").map(|v| v.to_string()),
            obj.get("version_status").map(|v| v.to_string()),
        ))
    } else {
        None
    }
}

fn insert_batch(conn: &mut Connection, batch: &[Row]) -> Result<()> {
    let tx = conn.transaction()?;
    let mut stmt = tx.prepare("INSERT INTO people VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)")?;

    for (
        id,
        full_name,
        gender,
        job_title,
        country,
        region,
        continent,
        job_last_updated,
        experience,
        education,
        profiles,
        version_status,
    ) in batch
    {
        stmt.execute(params![
            id.as_deref(),
            full_name.as_deref(),
            gender.as_deref(),
            job_title.as_deref(),
            country.as_deref(),
            region.as_deref(),
            continent.as_deref(),
            job_last_updated.as_deref(),
            experience.as_deref(),
            education.as_deref(),
            profiles.as_deref(),
            version_status.as_deref()
        ])?;
    }

    tx.commit()?;
    Ok(())
}
